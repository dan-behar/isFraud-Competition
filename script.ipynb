{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Fraud Analysis* Machine Learning Models Project\n",
    "## <font size=5 color='gray'>Daniel Behar</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to select and create the best prediction algorithm to predict whether a certain transaction is fraudulent or not. This is the first projecto of Machine Learning Models course.\n",
    "#### Structure of the notebook:\n",
    "* `Libraries`: Includes a briefly description of where they were used in the process\n",
    "* `Data Export`: Includes the data export, separation in train-test and exploration\n",
    "* `Cleaning Pipelines`: Building of the three pipelines that I used\n",
    "* `Model`: Building of the pipeline where I'm placing my model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General use libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#To split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Full Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Dates Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#Numeric Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Categoric Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Model\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "* Getting the data\n",
    "* Extracting isFraud from the Train set (Test doesn't have it)\n",
    "* Extracting id from the Test set (it will be needed later)\n",
    "* Creating train and test sets from the Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data will be the data that I'll use to create and train the model, datat is the data for which I want to predict the isFraud variable\n",
    "data = pd.read_csv(\"Train.csv\")\n",
    "datat = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.loc[:,\"isFraud\"]\n",
    "id = datat.loc[:,\"id\"]\n",
    "data.drop([\"id\", \"currentExpDate\", \"merchantName\", \"isFraud\", \"transactionDateTime\", \"accountOpenDate\", \"dateOfLastAddressChange\"],axis=1,inplace=True)\n",
    "datat.drop([\"id\", \"merchantName\", \"currentExpDate\", \"transactionDateTime\", \"accountOpenDate\", \"dateOfLastAddressChange\"],axis=1,inplace=True)\n",
    "print(data.shape)\n",
    "print(y.shape)\n",
    "print(datat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the model, this variables were not really useful, so I removed them:\n",
    "- currentExpDate\n",
    "- merchantName\n",
    "- isFraud\n",
    "- transactionDateTime\n",
    "- accountOpenDate\n",
    "- dateOfLastAddressChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data\n",
    "* Search for NAs values and not normal observations\n",
    "* Searching for relevant information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of NAs for each column\n",
    "nas = pd.DataFrame(data.isna().sum()/data.shape[0], columns = ['%NAs']).reset_index()\n",
    "nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(column):\n",
    "    return data[column].value_counts()\n",
    "\n",
    "unique(\"posEntryMode\")\n",
    "unique(\"posConditionCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "val = []\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    cols.append(col)\n",
    "    val.append(data[col].str.contains(r'/').sum())\n",
    "pd.DataFrame({\n",
    "    'cols':cols,\n",
    "    'val':val\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data.hist(figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = data.corr(method='pearson')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_df, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Trues and Falses in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(lst):\n",
    "    return sum(bool(x) for x in lst)\n",
    " \n",
    "# Driver code\n",
    "print(count(y))\n",
    "print(count(y)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(lst):\n",
    "    return sum(bool(x) for x in lst)\n",
    "\n",
    "print((count(y)/len(y))*100)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "y.value_counts().plot(kind='bar', title='True/False relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines to clean the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline([\n",
    "                            ('Imputador', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                            ('std_scaler', StandardScaler()),\n",
    "                        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "posEntryMode and posConditionCode are the only numeric values that have NAs, and because those are discrete variables the only imputer strategy valid is Mode, aka \"Most_frequent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = numeric_pipeline.fit_transform(data.select_dtypes(include='number'))\n",
    "numerical[0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoric Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_pipeline = Pipeline([\n",
    "                        ('Imputador', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                        ('ohe', OneHotEncoder())\n",
    "                        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the pipeline. Categoricas contains the raw columns, categorical contains the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = data[[\"acqCountry\", \"merchantCountryCode\", \"merchantCategoryCode\", \"transactionType\", \"cardPresent\", \"expirationDateKeyInMatch\"]]\n",
    "categorical = categoric_pipeline.fit_transform(categoricas.values)\n",
    "categorical.toarray()[0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date columns are string but I can't send them in the categorical pipeline because isn't useful to me\n",
    "- I need to pick specific string columns in order to clean correctly the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attributes = data.select_dtypes(include='number').columns \n",
    "categorical_attributes = data.select_dtypes(exclude='number').columns \n",
    "\n",
    "full_cleaning_pipeline = ColumnTransformer([\n",
    "        (\"numerics\", numeric_pipeline, numerical_attributes),\n",
    "        (\"categorics\", categoric_pipeline, categorical_attributes)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cleaning_pipeline.fit(data)\n",
    "ready_Xtrain = full_cleaning_pipeline.transform(X_train)\n",
    "ready_Xtest = full_cleaning_pipeline.transform(X_test)\n",
    "\n",
    "print(ready_Xtrain.shape)\n",
    "print(ready_Xtest.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_pipeline = Pipeline([\n",
    "        (\"data_preparation\", full_cleaning_pipeline),\n",
    "        (\"DTC\", DecisionTreeClassifier(random_state=10000))\n",
    "    ])\n",
    "\n",
    "predictor_pipeline.fit(X_train, y_train)\n",
    "predicted_vals = predictor_pipeline.predict(X_test)\n",
    "\n",
    "print('DecisionTreeClassifier:\\n')\n",
    "print('F1: {0}'.format(f1_score(y_test,predicted_vals,average='weighted')))\n",
    "print('Precision Score: {0}'.format(precision_score(y_test,predicted_vals,average='weighted')))\n",
    "print('Recall Score: {0}'.format(recall_score(y_test,predicted_vals, average='weighted')))\n",
    "cm = confusion_matrix(y_test,predicted_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model pipeline\n",
    "- Using datat (Test dataset) to predict the isFraud variable\n",
    "- Because I uploaded the answers to Kaggle, I need to merge the ID's of the transactions and the transactions result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = predictor_pipeline\n",
    "joblib.dump(DT_model, \"FraudModel.pkl\")\n",
    "FraudModel = joblib.load(\"FraudModel.pkl\")\n",
    "isFraud = FraudModel.predict(datat)\n",
    "solutions = pd.concat([id, pd.DataFrame(isFraud)], axis=1)\n",
    "solutions.to_csv('solutions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
